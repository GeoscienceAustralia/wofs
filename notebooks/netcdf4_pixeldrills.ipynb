{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy\n",
    "import netCDF4\n",
    "import rasterio\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "from osgeo import osr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Four netcdf files: \n",
    "#1 Old java optimised for pixel drill. \n",
    "#2 same lat,lon,time order as1 but 100x100x100 chucksize for compression\n",
    "#3 new order ubyte Data(time, lat, lon) with 100x100x100 chuncksize\n",
    "#4 new order ubyte Data(time, lat, lon) with 500x10x10 chuncksize\n",
    "ncfile1=\"/g/data/fk4/wofs/water_f7q/extents/149_-035/LS_WATER_149_-035_1987-07-16T23-15-17.556_2014-03-28T23-47-03.171.nc\" #1x256x1200\n",
    "ncfile2=\"/g/data/fk4/wofs/water_20160203/extents/149_-035/LS_WATER_149_-035_1987-07-16T23-15-17_2016-01-20T23-57-35.nc\" #100x100x100\n",
    "ncfile3=\"/g/data/u46/wofs/extents2nc/149_-035/LS_WATER_149_-035_1987-07-16T23-15-17_2016-01-20T23-57-35.nc\" #100x100x100\n",
    "ncfile4=\"/g/data/u46/wofs/extents/149_-035/LS_WATER_149_-035_1987-07-16T23-15-17_2016-01-20T23-57-35.nc\"  #500x10x10 chunk\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 277  100 3169 3279  663 1455  481 2411 2947 3083] [3129 3345 1624 3971 1200  612  718  627 1800 1817]\n"
     ]
    }
   ],
   "source": [
    "# make up 1000 pixels randomly\n",
    "npix=1000\n",
    "pixels1 = numpy.random.randint(0, 4000, (npix))\n",
    "pixels2 = numpy.random.randint(0, 4000, (npix))\n",
    "\n",
    "print pixels1[:10], pixels2[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read1(data): # time last\n",
    "    for i in range(1,len(pixels1)):\n",
    "        data[pixels1[i],pixels2[i],:]\n",
    "\n",
    "        \n",
    "def read2(data):  # time first\n",
    "    for i in range(1,len(pixels1)):\n",
    "        data[:, pixels1[i],pixels2[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETCDF4\n",
      "15456000000 int8 (4000, 4000, 966)\n",
      "The slowest run took 15.75 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "1 loops, best of 3: 1.28 s per loop\n"
     ]
    }
   ],
   "source": [
    "ds1=netCDF4.Dataset(ncfile1)\n",
    "\n",
    "print ds1.data_model\n",
    "#print ds1.variables\n",
    "\n",
    "data1=ds1['Data']\n",
    "print data1.size, data1.dtype, data1.shape\n",
    "\n",
    "%timeit read1(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETCDF4\n",
      "18336000000 int8 (4000, 4000, 1146)\n",
      "1 loops, best of 3: 39.6 s per loop\n"
     ]
    }
   ],
   "source": [
    "# 20-30 times slower compare to method1\n",
    "ds1=netCDF4.Dataset(ncfile2)\n",
    "data1=ds1['Data']\n",
    "\n",
    "print ds1.data_model\n",
    "#print ds1.variables\n",
    "\n",
    "print data1.size, data1.dtype, data1.shape\n",
    "\n",
    "\n",
    "%timeit read1(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETCDF4\n",
      "18336000000 uint8 (1146, 4000, 4000)\n",
      "1 loops, best of 3: 27.6 s per loop\n"
     ]
    }
   ],
   "source": [
    "# 20-30 times slower compare to method1\n",
    "ds1=netCDF4.Dataset(ncfile3)\n",
    "\n",
    "print ds1.data_model\n",
    "# ds1.variables\n",
    "\n",
    "data1=ds1['Data']\n",
    "print data1.size, data1.dtype, data1.shape\n",
    "\n",
    "%timeit read2(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETCDF4\n",
      "18336000000 uint8 (1146, 4000, 4000)\n",
      "The slowest run took 6.87 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "1 loops, best of 3: 694 ms per loop\n"
     ]
    }
   ],
   "source": [
    "# method-7 chucksize 500X10X10, time first is fastest for reading. half of method 1. Fastest\n",
    "# takes more RAM memory and cpu to create. But faster to drill\n",
    "# pus to limit, use len(times)X10x10 as chuncksize? \n",
    "ds1=netCDF4.Dataset(ncfile4)\n",
    "\n",
    "print ds1.data_model\n",
    "# ds1.variables\n",
    "\n",
    "data1=ds1['Data']\n",
    "print data1.size, data1.dtype, data1.shape\n",
    "\n",
    "#print data1[:, 1,1,]\n",
    "\n",
    "%timeit read2(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1516 2464 1356 2489 1667 2819  656 3650 1250 1052] [ 706 1698 2876 2963  927 3469  355 3695 2815 2513]\n"
     ]
    }
   ],
   "source": [
    "pixels1 = numpy.random.randint(0, 4000, (10))\n",
    "pixels2 = numpy.random.randint(0, 4000, (10))\n",
    "\n",
    "print pixels1, pixels2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read(data):\n",
    "    for i in range(1,len(pixels1)):\n",
    "        data[:,pixels1[i],pixels2[i]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 14.11 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "100 loops, best of 3: 3.1 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From: Sixsmith Joshua \n",
    "Sent: Wednesday, 23 March 2016 4:33 PM\n",
    "To: Zhang Fei\n",
    "Cc: Raevski Gregory\n",
    "Subject: RE: NetCDF file size [SEC=UNCLASSIFIED]\n",
    "\n",
    "Hi Fei,\n",
    "\n",
    "Here is the code snippet used to test reading a time series from a file.  Effects become more noticeable the more pixels you wish to read.  If an immediately following pixel is contained within the same chunk, then it should also read quicker due to caching.\n",
    "\n",
    "In [14]: pixels\n",
    "Out[14]:\n",
    "array([1829, 2675, 3626, 3514, 2950,  628, 1023, 1392, 1176,  162,   95,\n",
    "       3659, 3223,  490, 1161, 1519, 3541,  823, 3363, 1430, 1663, 1575,\n",
    "       1226,  227, 1448, 3763,   11, 1556, 1935, 1294, 3468, 3706,  821,\n",
    "       1931,  301,  889, 3912, 2881, 1250, 3231, 3421, 3999,  899, 1043,\n",
    "       2157, 3461, 2198, 1137, 3178, 2952, 3011, 2678, 2702,  546, 2347,\n",
    "       2089,  243,  609, 2506,  132,  227, 2692, 2965, 3613, 1725, 2229,\n",
    "       1576, 1993,  559, 1817, 1679, 3240, 3867, 1300,  863, 1729,  211,\n",
    "       3222, 1271, 3892, 2402, 3527, 3862, 1050, 2769,  320, 3591, 1303,\n",
    "       2998, 3176,  539,  710, 3170, 3741, 1408, 3524, 2342, 1579, 2664,\n",
    "       2358])\n",
    "\n",
    "In [15]: def read1(ds, pixels):\n",
    "   ....:     for pix in pixels:\n",
    "   ....:         val = ds[pix, pix, :]\n",
    "   ....:\n",
    "\n",
    "In [16]: def read2(ds, pixels):\n",
    "   ....:     for pix in pixels:\n",
    "   ....:         val = ds[:, pix, pix]\n",
    "   ....:\n",
    "\n",
    "In [17]: %timeit read1(d1, pixels)\n",
    "1 loops, best of 3: 129 ms per loop\n",
    "\n",
    "In [18]: %timeit read2(d2, pixels)\n",
    "1 loops, best of 3: 1.3 s per loop\n",
    "\n",
    "In [19]: d1.chunks\n",
    "Out[19]: (1, 250, 1046)\n",
    "\n",
    "In [20]: pixels = numpy.random.randint(0, 4000, (1000))\n",
    "\n",
    "In [21]: %timeit read1(d1, pixels)\n",
    "1 loops, best of 3: 1.29 s per loop\n",
    "\n",
    "In [22]: %timeit read2(d2, pixels)\n",
    "1 loops, best of 3: 14.2 s per loop\n",
    "\n",
    "\n",
    "The dimensional ordering for “d1” is (y, x, z) and the dimensional ordering for “d2” is (z, y, x).  The chunks for “d1” are (1, 250, 1046), and the chunks for “d2” are (100, 100, 100).\n",
    "\n",
    "So in order to read a time series for “d1” a single chunk is read i.e. (1, 250, 1046), containing 261,500 elements.  Also, there is less skipping on the memory order to form the time series as the time dimension “z”, is already in contiguous memory.\n",
    "In order to read a time series for “d2”, 11 x (100, 100, 100) blocks are read, which totals 11,000,000 elements.  At each chunk read, the data array also needs to skip over (100x*100y) elements in order to form a time series which (minutely) adds to the overall read time.\n",
    "\n",
    "Hope that helps\n",
    "\n",
    "Cheers\n",
    "Josh\n",
    "\n",
    "From: Zhang Fei \n",
    "Sent: Wednesday, 23 March 2016 11:05 AM\n",
    "To: Sixsmith Joshua\n",
    "Cc: Raevski Gregory; Ayers Damien; Oliver Simon; Wu Wenjun; Hicks Andrew; Hooke Jeremy; Bala Biswajit\n",
    "Subject: NetCDF file size [SEC=UNCLASSIFIED]\n",
    "Importance: High\n",
    "\n",
    "\n",
    "Hi Josh and all,\n",
    "\n",
    "For pixel drill purpose, we needed to stack the WOFS extents tiff files into netCDF files, one for each cell. \n",
    "\n",
    "I have observed the following file size phenomena. \n",
    "If you can study further and give some explanation It would be great.\n",
    "\n",
    "Cheers\n",
    "\n",
    "Fei\n",
    "\n",
    "Create NetCDF stacks: comparison of 3 methods\n",
    "\n",
    "Water extents tiff files    /g/data/u46/wofs/extents \n",
    "total size = 514 GB.  \n",
    "\n",
    "Method\tTotal netcdf files size\tComputing Time for netcdf files creation\tprogram\tInteroperability.\tSource code and netcdf files\t\n",
    "1.Java-based\t2100 GB\tWorks done 1.5 years ago. Not sure. Heard very slow\tComplex java libraries dependency, Not working now.\tGdalinfo does not like the Netcdf file\tls /g/data/fk4/wofs/water_f7q/extents/*/*.nc \n",
    "\n",
    "du –ch /g/data/fk4/wofs/water_f7q/extents/*/*.nc\t\n",
    "2.Python\n",
    "netCDF files are\n",
    "non-CF compliant\t1001 GB\t100 hours\n",
    "(2cpu)\tPython-based.\n",
    "A  200-line script\tSame as above Gdalinfo does not like the Netcdf file\tls /g/data/fk4/wofs/water_20160203/extents/*/*.nc \n",
    "\n",
    "du –ch /g/data/fk4/wofs/water_20160203/extents/*/*.nc\n",
    "https://github.com/feizhanga/scicomput/blob/master/GeoDataSoft/netCDF4/stack_tiffs2netcdf.py \n",
    "\n",
    "3.New Python CF1.6 compliant netCDF4\t370 GB\t47 hours\n",
    "(2cpu)\tPython-based.\n",
    "A  200-line script\tUpdated netCDF format, interoperable with other tools such as GDAL, ncdump, etc.\tls /g/data/u46/wofs/extents2nc/*/*.nc \n",
    "\n",
    "du –ch /g/data/u46/wofs/extents2nc/*/*.nc\n",
    "\n",
    "https://github.com/feizhanga/scicomput/blob/master/GeoDataSoft/netCDF4/stack_tiffs2netcdf_CF.py\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
